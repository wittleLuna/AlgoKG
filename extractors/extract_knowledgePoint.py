import spacy
import json
import re
import os
import uuid
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from openai import AsyncOpenAI
from dotenv import load_dotenv
import asyncio
from enum import Enum
import aiohttp

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å®ä½“ç±»åˆ«æšä¸¾
class EntityLabels(str, Enum):
    ALGORITHM_PARADIGM = "algorithm_paradigm"
    DATA_STRUCTURE = "data_structure"
    SPECIFIC_ALGORITHM = "specific_algorithm"
    PROBLEM_TYPE = "problem_type"
    CORE_CONCEPT = "core_concept"
    COMPLEXITY = "complexity"
    TECHNIQUE = "technique"

@dataclass
class KnowledgePoint:
    id: str
    name: str
    alias: List[str]
    type: str
    description: str
    key_concepts: List[str]
    applications: List[str]
    created_at: str = None
    updated_at: str = None
    source_files: Optional[List[str]] = None
    spacy_entities: Optional[Dict[str, List[str]]] = None
    
    def to_dict(self):
        result = {
            "id": self.id,
            "name": self.name,
            "alias": self.alias,
            "type": self.type,
            "description": self.description,
            "key_concepts": self.key_concepts,
            "applications": self.applications,
            "created_at": self.created_at or datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "updated_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ"),
            "source_files": self.source_files or [],
            "spacy_entities": self.spacy_entities or {}
        }
        return result

class QwenClient:
    """Qwenæ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str = None, base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"):
        self.api_key = api_key or os.getenv("DASHSCOPE_API_KEY")
        self.base_url = base_url
        self.async_client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

    async def chat_completion_async(self, messages: List[Dict], model: str = "qwen-max") -> str:
        try:
            response = await self.async_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.1,
                max_tokens=2000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ Qwen APIå¼‚æ­¥è°ƒç”¨å¤±è´¥: {e}")
            return "{}"
            

# å…œåº•ï¼šæ— è®ºæ¨¡å‹æ˜¯å¦ä»è¾“å‡ºï¼Œç»Ÿä¸€æ¸…ç† <think>...</think>ï¼ˆè·¨è¡Œ/å¤§å°å†™ï¼‰
_THINK_RE = re.compile(r"<\s*think\s*>.*?<\s*/\s*think\s*>\s*", flags=re.S | re.I)

def strip_think(text: str) -> str:
    return _THINK_RE.sub("", text).strip()

class QwenClientNative:
    """åŸç”Ÿ Ollama /api/chat å®¢æˆ·ç«¯ï¼ˆå¼‚æ­¥ï¼‰
    - ä»æºå¤´: options.think = False å…³é—­æ€è€ƒè¾“å‡º
    """

    def __init__(
        self,
        host: str | None = None,
        default_model: str = "qwen3:8b",
        timeout: int = 120,
        clean_output: bool = True,
        inject_no_think: bool = True,
    ):
        # ä¼˜å…ˆç¯å¢ƒå˜é‡
        env_host = os.getenv("OLLAMA_BASE_URL")
        self.host = (host or env_host or "http://127.0.0.1:11434").rstrip("/")
        self.default_model = default_model
        self.timeout = timeout
        self.clean_output = clean_output
        self.inject_no_think = inject_no_think

    async def chat_completion_async(
        self,
        messages: List[Dict],
        model: Optional[str] = None,
        temperature: float = 0.2,
        max_tokens: int = 1024,
        stream: bool = False,
        keepalive: Optional[int] = None,  # å¯é€‰ï¼šç»™æ¨¡å‹ä¿æ´»ç§’æ•°ï¼ˆä¸æƒ³é‡Šæ”¾æ˜¾å­˜ï¼‰
    ) -> str:
        """è°ƒç”¨åŸç”Ÿ /api/chatï¼Œè¿”å›å­—ç¬¦ä¸²ç»“æœ"""
        url = f"{self.host}/api/chat"

        # å¯é€‰ï¼šåœ¨æ¶ˆæ¯æœ€å‰æ³¨å…¥ system æŒ‡ä»¤ï¼Œè¿›ä¸€æ­¥å‹åˆ¶ think è¾“å‡º
        final_messages = messages
        if self.inject_no_think:
            sys_msg = {
                "role": "system",
                "content": "è¯·åªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ– <think> æ ‡ç­¾ã€‚"
            }
            if not messages or messages[0].get("role") != "system":
                final_messages = [sys_msg] + messages

        payload: Dict = {
            "model": model or self.default_model,
            "messages": final_messages,
            "stream": stream,
            # åŸç”Ÿ optionsï¼šè¿™é‡Œçš„ think=False æ‰ä¼šè¢«è¯†åˆ«
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "think": False
            }
        }
        if keepalive is not None:
            # å‘Šè¯‰ Ollama åœ¨æœ¬æ¬¡è°ƒç”¨åä¿ç•™æ¨¡å‹ keepalive ç§’ï¼ˆå‡å°‘å†·å¯åŠ¨ï¼‰
            payload["keep_alive"] = f"{int(keepalive)}s"

        timeout = aiohttp.ClientTimeout(total=self.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(url, json=payload) as r:
                r.raise_for_status()

                if stream:
                    # æµå¼ï¼šæŠŠç‰‡æ®µå…ˆç¼“å†²èµ·æ¥ï¼Œæœ€åç»Ÿä¸€æ¸…æ´—ï¼ˆé¿å…thinkè¢«åˆ†å—æ‹†å¼€ï¼‰
                    buf: List[str] = []
                    async for line in r.content:
                        if not line:
                            continue
                        try:
                            data = json.loads(line.decode("utf-8"))
                        except Exception:
                            continue
                        part = data.get("message", {}).get("content", "")
                        if part:
                            buf.append(part)
                        if data.get("done"):
                            break
                    text = "".join(buf)
                else:
                    data = await r.json()
                    text = data.get("message", {}).get("content", "") or ""

        return strip_think(text) if self.clean_output else text


class PreciseEntityExtractor:
    """ç²¾ç¡®çš„å®ä½“æå–å™¨ - ä¸“æ³¨äºLLMå‡†ç¡®æ€§"""
    
    def __init__(self, qwen_api_key: str = None, pattern_path: str = "patterns.json"):
        # åˆå§‹åŒ–spaCy
        self.nlp = spacy.load("zh_core_web_sm")
        
        # ç¦ç”¨é»˜è®¤NERï¼Œé¿å…é€šç”¨å®ä½“å¹²æ‰°
        if "ner" in self.nlp.pipe_names:
            self.nlp.remove_pipe("ner")
            print("ğŸš« å·²ç¦ç”¨spaCyé»˜è®¤NERï¼Œé¿å…é€šç”¨å®ä½“å¹²æ‰°")
        
        # æ·»åŠ å®ä½“è§„åˆ™å™¨
        self.ruler = self.nlp.add_pipe("entity_ruler")
        self.setup_patterns(pattern_path)
        
        self.qwen_client = QwenClient(api_key=qwen_api_key)
        self.current_version = "v2.1"
        
        # åˆå§‹åŒ–ç²¾ç¡®è¯å…¸
        self.init_precise_dictionaries()

    def setup_patterns(self, config_path="patterns.json"):
        """è®¾ç½®spaCyæ¨¡å¼"""
        if os.path.exists(config_path):
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    patterns = json.load(f)
                self.ruler.add_patterns(patterns)
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å¼æ–‡ä»¶: {config_path}ï¼Œå…± {len(patterns)} ä¸ªæ¨¡å¼")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å¼æ–‡ä»¶å¤±è´¥: {e}")
                self.setup_default_patterns()
        else:
            print(f"âš ï¸ æ¨¡å¼æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼")
            self.setup_default_patterns()

    def setup_default_patterns(self):
        """è®¾ç½®é»˜è®¤çš„spaCyæ¨¡å¼"""
        default_patterns = [
            # ç®—æ³•èŒƒå¼
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "åŠ¨æ€è§„åˆ’"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "dp"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "è´ªå¿ƒ"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "åˆ†æ²»"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "å›æº¯"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "dfs"}]},
            {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "bfs"}]},
            
            # æ•°æ®ç»“æ„
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ•°ç»„"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "é“¾è¡¨"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ ˆ"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "é˜Ÿåˆ—"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ ‘"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "å›¾"}]},
            {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "å“ˆå¸Œè¡¨"}]},
            
            # å…·ä½“ç®—æ³•
            {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "å¿«é€Ÿæ’åº"}]},
            {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "å½’å¹¶æ’åº"}]},
            {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "äºŒåˆ†æŸ¥æ‰¾"}]},
            {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "kmp"}]},
            
            # æŠ€å·§
            {"label": "TECHNIQUE", "pattern": [{"LOWER": "åŒæŒ‡é’ˆ"}]},
            {"label": "TECHNIQUE", "pattern": [{"LOWER": "æ»‘åŠ¨çª—å£"}]},
            {"label": "TECHNIQUE", "pattern": [{"LOWER": "å‰ç¼€å’Œ"}]},
            
            # æ ¸å¿ƒæ¦‚å¿µ
            {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "æ—¶é—´å¤æ‚åº¦"}]},
            {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "ç©ºé—´å¤æ‚åº¦"}]},
            {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "é€’å½’"}]},
        ]
        
        self.ruler.add_patterns(default_patterns)
        print(f"âœ… åŠ è½½é»˜è®¤æ¨¡å¼ï¼Œå…± {len(default_patterns)} ä¸ªæ¨¡å¼")

    def init_precise_dictionaries(self):
        """åˆå§‹åŒ–ç²¾ç¡®è¯å…¸"""
        
        # é—®é¢˜ç±»å‹è¯å…¸ - æœ€é‡è¦ï¼Œéœ€è¦æœ€ç²¾ç¡®
        self.problem_type_dict = {
            # èƒŒåŒ…ç±»é—®é¢˜
            '01èƒŒåŒ…é—®é¢˜', '0-1èƒŒåŒ…é—®é¢˜', 'å®Œå…¨èƒŒåŒ…é—®é¢˜', 'å¤šé‡èƒŒåŒ…é—®é¢˜', 'æ··åˆèƒŒåŒ…é—®é¢˜', 'èƒŒåŒ…é—®é¢˜',
            # è·¯å¾„ç±»é—®é¢˜  
            'æœ€çŸ­è·¯å¾„é—®é¢˜', 'æœ€é•¿è·¯å¾„é—®é¢˜', 'å•æºæœ€çŸ­è·¯å¾„é—®é¢˜', 'å¤šæºæœ€çŸ­è·¯å¾„é—®é¢˜',
            # åºåˆ—ç±»é—®é¢˜
            'æœ€é•¿å…¬å…±å­åºåˆ—é—®é¢˜', 'æœ€é•¿é€’å¢å­åºåˆ—é—®é¢˜', 'æœ€é•¿å…¬å…±å­ä¸²é—®é¢˜', 'æœ€é•¿å›æ–‡å­åºåˆ—é—®é¢˜',
            # åŒ¹é…ç±»é—®é¢˜
            'å­—ç¬¦ä¸²åŒ¹é…é—®é¢˜', 'æ¨¡å¼åŒ¹é…é—®é¢˜', 'æ­£åˆ™åŒ¹é…é—®é¢˜',
            # éå†ç±»é—®é¢˜
            'å›¾éå†é—®é¢˜', 'æ ‘éå†é—®é¢˜', 'äºŒå‰æ ‘éå†é—®é¢˜',
            # åŸºç¡€é—®é¢˜ç±»å‹
            'æ’åºé—®é¢˜', 'æŸ¥æ‰¾é—®é¢˜', 'æœç´¢é—®é¢˜',
            # ç‰¹æ®Šé—®é¢˜
            'è¿é€šæ€§é—®é¢˜', 'ç¯æ£€æµ‹é—®é¢˜', 'æ‹“æ‰‘æ’åºé—®é¢˜',
            'åŒºé—´è°ƒåº¦é—®é¢˜', 'ä»»åŠ¡è°ƒåº¦é—®é¢˜', 'ä½œä¸šè°ƒåº¦é—®é¢˜',
            # ç»å…¸é—®é¢˜
            'æ–æ³¢é‚£å¥‘é—®é¢˜', 'çˆ¬æ¥¼æ¢¯é—®é¢˜', 'ç¡¬å¸é—®é¢˜', 'åˆ‡å‰²é—®é¢˜', 'è‚¡ç¥¨é—®é¢˜'
        }
        
        # å¤æ‚åº¦è¯å…¸ - æ ‡å‡†æ ¼å¼
        self.complexity_dict = {
            'O(1)', 'O(log n)', 'O(logn)', 'O(n)', 'O(n log n)', 'O(nlogn)', 
            'O(nÂ²)', 'O(n^2)', 'O(nÂ³)', 'O(n^3)', 'O(2^n)', 'O(n!)', 
            'O(mn)', 'O(m*n)', 'O(m+n)', 'O(V+E)', 'O(ElogV)'
        }
        
        # ç®—æ³•èŒƒå¼è¯å…¸
        self.algorithm_paradigm_dict = {
            'åŠ¨æ€è§„åˆ’', 'DP', 'è´ªå¿ƒç®—æ³•', 'è´ªå¿ƒç­–ç•¥', 'åˆ†æ²»ç®—æ³•', 'åˆ†æ²»æ³•',
            'å›æº¯ç®—æ³•', 'å›æº¯æ³•', 'æ·±åº¦ä¼˜å…ˆæœç´¢', 'DFS', 'å¹¿åº¦ä¼˜å…ˆæœç´¢', 'BFS',
            'æšä¸¾ç®—æ³•', 'æš´åŠ›æœç´¢', 'é€’å½’ç®—æ³•', 'è¿­ä»£ç®—æ³•'
        }
        
        # å…·ä½“ç®—æ³•è¯å…¸
        self.specific_algorithm_dict = {
            'KMPç®—æ³•', 'KMP', 'å¿«é€Ÿæ’åº', 'å½’å¹¶æ’åº', 'å †æ’åº', 'å†’æ³¡æ’åº', 'æ’å…¥æ’åº', 'é€‰æ‹©æ’åº',
            'äºŒåˆ†æŸ¥æ‰¾', 'äºŒåˆ†æœç´¢', 'Dijkstraç®—æ³•', 'Floydç®—æ³•', 'Primç®—æ³•', 'Kruskalç®—æ³•',
            'A*ç®—æ³•', 'Bellman-Fordç®—æ³•', 'æ‹“æ‰‘æ’åºç®—æ³•', 'æœ€å°ç”Ÿæˆæ ‘ç®—æ³•'
        }
        
        # æ•°æ®ç»“æ„è¯å…¸
        self.data_structure_dict = {
            'æ•°ç»„', 'é“¾è¡¨', 'å•é“¾è¡¨', 'åŒé“¾è¡¨', 'ç¯å½¢é“¾è¡¨',
            'æ ˆ', 'é˜Ÿåˆ—', 'ä¼˜å…ˆé˜Ÿåˆ—', 'åŒç«¯é˜Ÿåˆ—',
            'æ ‘', 'äºŒå‰æ ‘', 'äºŒå‰æœç´¢æ ‘', 'å¹³è¡¡æ ‘', 'AVLæ ‘', 'çº¢é»‘æ ‘',
            'å›¾', 'æœ‰å‘å›¾', 'æ— å‘å›¾', 'åŠ æƒå›¾',
            'å“ˆå¸Œè¡¨', 'æ•£åˆ—è¡¨', 'å­—å…¸', 'é›†åˆ',
            'å †', 'æœ€å¤§å †', 'æœ€å°å †', 'äºŒå‰å †',
            'å­—å…¸æ ‘', 'Trieæ ‘', 'å‰ç¼€æ ‘'
        }
        
        # æŠ€å·§è¯å…¸
        self.technique_dict = {
            'åŒæŒ‡é’ˆ', 'å¿«æ…¢æŒ‡é’ˆ', 'å·¦å³æŒ‡é’ˆ', 'å¯¹æ’æŒ‡é’ˆ',
            'æ»‘åŠ¨çª—å£', 'å›ºå®šçª—å£', 'å¯å˜çª—å£',
            'å‰ç¼€å’Œ', 'å‰ç¼€ç§¯', 'å·®åˆ†æ•°ç»„', 'äºŒç»´å‰ç¼€å’Œ',
            'å•è°ƒæ ˆ', 'å•è°ƒé€’å¢æ ˆ', 'å•è°ƒé€’å‡æ ˆ', 'å•è°ƒé˜Ÿåˆ—',
            'å‰ç¼€è¡¨', 'nextæ•°ç»„', 'Zç®—æ³•',
            'çŠ¶æ€å‹ç¼©', 'ä½è¿ç®—ä¼˜åŒ–', 'è®°å¿†åŒ–æœç´¢', 'è®°å¿†åŒ–',
            'å¹¶æŸ¥é›†', 'è·¯å¾„å‹ç¼©', 'æŒ‰ç§©åˆå¹¶',
            'çº¿æ®µæ ‘', 'æ ‘çŠ¶æ•°ç»„', 'åˆ†å—ç®—æ³•',
            'å“ˆå¸ŒæŠ€å·§', 'åŒå“ˆå¸Œ', 'æ»šåŠ¨å“ˆå¸Œ'
        }
        
        # æ ¸å¿ƒæ¦‚å¿µè¯å…¸
        self.core_concept_dict = {
            'æ—¶é—´å¤æ‚åº¦', 'ç©ºé—´å¤æ‚åº¦', 'æ¸è¿›å¤æ‚åº¦',
            'é€’å½’', 'è¿­ä»£', 'åˆ†æ²»', 'è´ªå¿ƒç­–ç•¥',
            'æœ€ä¼˜å­ç»“æ„', 'é‡å å­é—®é¢˜', 'çŠ¶æ€è½¬ç§»æ–¹ç¨‹', 'è¾¹ç•Œæ¡ä»¶',
            'çŠ¶æ€å®šä¹‰', 'çŠ¶æ€è½¬ç§»', 'åˆå§‹åŒ–', 'éå†é¡ºåº',
            'åŠ¨æ€è§„åˆ’', 'è®°å¿†åŒ–', 'è‡ªé¡¶å‘ä¸‹', 'è‡ªåº•å‘ä¸Š',
            'å›æº¯ç®—æ³•', 'å‰ªæä¼˜åŒ–', 'æœç´¢ç©ºé—´', 'è§£ç©ºé—´',
            'è´ªå¿ƒé€‰æ‹©', 'å±€éƒ¨æœ€ä¼˜', 'å…¨å±€æœ€ä¼˜'
        }

    def extract_by_precise_dictionary(self, text: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨ç²¾ç¡®è¯å…¸åŒ¹é…"""
        entities = {}
        
        # é—®é¢˜ç±»å‹åŒ¹é… - æœ€ä¸¥æ ¼
        found_problems = []
        for problem in self.problem_type_dict:
            if problem in text:
                found_problems.append(problem)
        if found_problems:
            entities["PROBLEM_TYPE"] = found_problems
        
        # å¤æ‚åº¦åŒ¹é… - æ ‡å‡†æ ¼å¼
        found_complexities = []
        for complexity in self.complexity_dict:
            variants = [complexity, complexity.replace(' ', ''), complexity.replace('Â²', '^2'), complexity.replace('Â³', '^3')]
            for variant in variants:
                if variant in text and complexity not in found_complexities:
                    found_complexities.append(complexity)
                    break
        if found_complexities:
            entities["COMPLEXITY"] = found_complexities
        
        # ç®—æ³•èŒƒå¼åŒ¹é…
        found_paradigms = []
        for paradigm in self.algorithm_paradigm_dict:
            if paradigm in text:
                found_paradigms.append(paradigm)
        if found_paradigms:
            entities["ALGORITHM_PARADIGM"] = found_paradigms
        
        # å…·ä½“ç®—æ³•åŒ¹é…
        found_algorithms = []
        for algorithm in self.specific_algorithm_dict:
            if algorithm in text:
                found_algorithms.append(algorithm)
        if found_algorithms:
            entities["SPECIFIC_ALGORITHM"] = found_algorithms
        
        # æ•°æ®ç»“æ„åŒ¹é…
        found_structures = []
        for structure in self.data_structure_dict:
            if structure in text:
                found_structures.append(structure)
        if found_structures:
            entities["DATA_STRUCTURE"] = found_structures
        
        # æŠ€å·§åŒ¹é…
        found_techniques = []
        for technique in self.technique_dict:
            if technique in text:
                found_techniques.append(technique)
        if found_techniques:
            entities["TECHNIQUE"] = found_techniques
        
        # æ ¸å¿ƒæ¦‚å¿µåŒ¹é…
        found_concepts = []
        for concept in self.core_concept_dict:
            if concept in text:
                found_concepts.append(concept)
        if found_concepts:
            entities["CORE_CONCEPT"] = found_concepts
        
        return entities

    def extract_with_spacy_rules(self, text: str) -> Dict[str, List[str]]:
        """ä½¿ç”¨spaCyè§„åˆ™æå–"""
        doc = self.nlp(text)
        entities = {}
        
        # å®šä¹‰ç®—æ³•ç›¸å…³æ ‡ç­¾
        algorithm_labels = {
            "ALGORITHM_PARADIGM", "DATA_STRUCTURE", "SPECIFIC_ALGORITHM",
            "PROBLEM_TYPE", "CORE_CONCEPT", "COMPLEXITY", "TECHNIQUE"
        }
        
        for ent in doc.ents:
            label = ent.label_
            if label in algorithm_labels:
                if label not in entities:
                    entities[label] = []
                if ent.text not in entities[label]:
                    entities[label].append(ent.text)
        
        return entities

    def clean_entities(self, entities: Dict[str, List[str]]) -> Dict[str, List[str]]:
        """æ¸…ç†æå–çš„å®ä½“"""
        cleaned_entities = {}
        
        for entity_type, entity_list in entities.items():
            cleaned_list = []
            
            for entity in entity_list:
                entity = entity.strip()
                
                # åŸºæœ¬é•¿åº¦è¿‡æ»¤
                if not (2 <= len(entity) <= 20):
                    continue
                
                # é’ˆå¯¹é—®é¢˜ç±»å‹çš„ç‰¹æ®Šè¿‡æ»¤
                if entity_type == "PROBLEM_TYPE":
                    # ä¸¥æ ¼è¿‡æ»¤é—®é¢˜ç±»å‹
                    if (3 <= len(entity) <= 12 and
                        not any(word in entity for word in ["æ‰€ä»¥", "é‡ç‚¹", "å°±æ˜¯", "å…¶å®", "éœ€è¦", "é€šè¿‡", "è¿™æ˜¯", "ä¹Ÿå°±æ˜¯", "è®²è§£", "å¼€å§‹", "æ ‡å‡†", "çº¯", "å…ˆ", "å¦‚æœ", "å› ä¸º"]) and
                        not entity.startswith(("è¿™", "ä¹Ÿ", "æ‰€", "å…¶", "å¦‚", "å› ")) and
                        any(keyword in entity for keyword in ["é—®é¢˜", "èƒŒåŒ…", "è·¯å¾„", "åºåˆ—", "åŒ¹é…", "æ’åº", "æŸ¥æ‰¾", "éå†"])):
                        cleaned_list.append(entity)
                
                # é’ˆå¯¹å¤æ‚åº¦çš„ç‰¹æ®Šè¿‡æ»¤
                elif entity_type == "COMPLEXITY":
                    if entity.startswith("O(") and entity.endswith(")"):
                        cleaned_list.append(entity)
                
                # å…¶ä»–ç±»å‹çš„åŸºæœ¬è¿‡æ»¤
                else:
                    if not any(word in entity for word in ["æ‰€ä»¥", "é‡ç‚¹", "å°±æ˜¯", "å…¶å®", "éœ€è¦", "é€šè¿‡", "è¿™æ˜¯", "ä¹Ÿå°±æ˜¯"]):
                        cleaned_list.append(entity)
            
            if cleaned_list:
                # å»é‡
                cleaned_entities[entity_type] = list(set(cleaned_list))
        
        return cleaned_entities

    async def enhance_with_precise_llm(self, text: str, spacy_entities: Dict[str, List[str]]) -> Dict:
        """ä½¿ç”¨LLMè¿›è¡Œç²¾ç¡®å¢å¼º"""
        
        # è¿‡æ»¤spaCyç»“æœ
        filtered_spacy = self.clean_entities(spacy_entities)
        
        spacy_summary = ""
        if filtered_spacy:
            spacy_summary = "\nå·²é€šè¿‡è§„åˆ™æå–åˆ°çš„å®ä½“ï¼š"
            for label, entities in filtered_spacy.items():
                spacy_summary += f"\n- {label}: {', '.join(entities[:5])}"  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸“ä¸šçš„ç®—æ³•çŸ¥è¯†ç‚¹æå–ä¸“å®¶ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ç²¾ç¡®è¯†åˆ«ç®—æ³•æ–‡æœ¬ä¸­çš„å®ä½“ï¼Œç‰¹åˆ«æ˜¯é—®é¢˜ç±»å‹å¿…é¡»æ˜¯æ ‡å‡†çš„åè¯æ€§çŸ­è¯­ã€‚

æ ¸å¿ƒåŸåˆ™ï¼š
1. é—®é¢˜ç±»å‹(PROBLEM_TYPE)å¿…é¡»æ˜¯è§„èŒƒçš„é—®é¢˜åç§°ï¼Œå¦‚"01èƒŒåŒ…é—®é¢˜"ã€"æœ€çŸ­è·¯å¾„é—®é¢˜"
2. ç»å¯¹ä¸è¦æå–åŒ…å«"æ‰€ä»¥"ã€"é‡ç‚¹"ã€"å°±æ˜¯"ç­‰è§£é‡Šè¯çš„å¥å­ç‰‡æ®µ
3. å¤æ‚åº¦å¿…é¡»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œå¦‚"O(n)"ã€"O(nÂ²)"
4. ä¸“æ³¨äºç®—æ³•é¢†åŸŸçš„æ ¸å¿ƒæœ¯è¯­ï¼Œé¿å…é€šç”¨è¯æ±‡
5. æ¯ä¸ªç±»åˆ«åªæå–æœ€é‡è¦ã€æœ€å‡†ç¡®çš„å®ä½“"""
            },
            {
                "role": "user",
                "content": f"""
è¯·ç²¾ç¡®åˆ†æä»¥ä¸‹ç®—æ³•æ–‡æœ¬ï¼Œæå–å…¶ä¸­çš„ä¸“ä¸šå®ä½“ã€‚é‡ç‚¹å…³æ³¨PROBLEM_TYPEçš„å‡†ç¡®æ€§ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text[:1200]}

{spacy_summary}

æå–è¦æ±‚ï¼š

**PROBLEM_TYPEï¼ˆé—®é¢˜ç±»å‹ï¼‰- æœ€é‡è¦**ï¼š
- å¿…é¡»æ˜¯æ ‡å‡†é—®é¢˜åç§°ï¼šå¦‚"01èƒŒåŒ…é—®é¢˜"ã€"æœ€çŸ­è·¯å¾„é—®é¢˜"ã€"æœ€é•¿å…¬å…±å­åºåˆ—é—®é¢˜"
- é•¿åº¦3-12ä¸ªå­—ç¬¦
- ä¸èƒ½åŒ…å«è§£é‡Šæ€§æ–‡å­—æˆ–å®Œæ•´å¥å­
- ä¸èƒ½ä»¥"è¿™"ã€"æ‰€ä»¥"ã€"é‡ç‚¹"ç­‰å¼€å¤´

**å…¶ä»–å®ä½“ç±»å‹**ï¼š
- ALGORITHM_PARADIGMï¼šåŠ¨æ€è§„åˆ’ã€è´ªå¿ƒç®—æ³•ã€åˆ†æ²»ç®—æ³•ç­‰
- SPECIFIC_ALGORITHMï¼šå¿«é€Ÿæ’åºã€KMPç®—æ³•ã€Dijkstraç®—æ³•ç­‰  
- DATA_STRUCTUREï¼šæ•°ç»„ã€é“¾è¡¨ã€æ ‘ã€å›¾ã€å“ˆå¸Œè¡¨ç­‰
- TECHNIQUEï¼šåŒæŒ‡é’ˆã€æ»‘åŠ¨çª—å£ã€å‰ç¼€å’Œã€å•è°ƒæ ˆç­‰
- CORE_CONCEPTï¼šæ—¶é—´å¤æ‚åº¦ã€çŠ¶æ€è½¬ç§»ã€æœ€ä¼˜å­ç»“æ„ç­‰
- COMPLEXITYï¼šO(n)ã€O(nÂ²)ã€O(logn)ç­‰æ ‡å‡†æ ¼å¼

è¯·è¿”å›JSONæ ¼å¼ï¼š

{{
  "name": "ä¸»è¦çŸ¥è¯†ç‚¹åç§°",
  "alias": ["åˆ«å1", "åˆ«å2"],
  "type": "algorithm_paradigm/data_structure/specific_algorithm/problem_type/core_concept/technique",
  "description": "è¯¦ç»†æè¿°",
  "key_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"],
  "applications": ["åº”ç”¨åœºæ™¯1", "åº”ç”¨åœºæ™¯2"],
  "refined_entities": {{
    "problem_type": ["æ ‡å‡†é—®é¢˜ç±»å‹åç§°"],
    "algorithm_paradigm": ["ç®—æ³•èŒƒå¼"],
    "specific_algorithm": ["å…·ä½“ç®—æ³•"],
    "data_structure": ["æ•°æ®ç»“æ„"],
    "technique": ["æŠ€å·§æ–¹æ³•"],
    "core_concept": ["æ ¸å¿ƒæ¦‚å¿µ"],
    "complexity": ["æ ‡å‡†å¤æ‚åº¦æ ¼å¼"]
  }}
}}

æ³¨æ„ï¼šå¦‚æœæŸä¸ªç±»åˆ«æ²¡æœ‰åˆé€‚çš„å®ä½“ï¼Œè¯·ç•™ç©ºæ•°ç»„ã€‚ç¡®ä¿æ¯ä¸ªå®ä½“éƒ½æ˜¯å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­ã€‚

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
            }
        ]
        
        response = await self.qwen_client.chat_completion_async(messages, model="qwen-max")
        
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                
                # ä¸¥æ ¼çš„åå¤„ç†éªŒè¯
                if "refined_entities" in result:
                    validated_entities = {}
                    for entity_type, entity_list in result["refined_entities"].items():
                        validated_list = []
                        for entity in entity_list:
                            if isinstance(entity, str):
                                entity = entity.strip()
                                
                                # é—®é¢˜ç±»å‹ä¸¥æ ¼éªŒè¯
                                if entity_type == "problem_type":
                                    if (3 <= len(entity) <= 12 and
                                        not any(word in entity for word in ["æ‰€ä»¥", "é‡ç‚¹", "å°±æ˜¯", "å…¶å®", "è¿™æ˜¯", "ä¹Ÿå°±æ˜¯", "å¦‚æœ", "å› ä¸º", "éœ€è¦", "é€šè¿‡", "è®²è§£"]) and
                                        any(keyword in entity for keyword in ["é—®é¢˜", "èƒŒåŒ…", "è·¯å¾„", "åºåˆ—", "åŒ¹é…", "æ’åº", "æŸ¥æ‰¾", "éå†"])):
                                        validated_list.append(entity)
                                
                                # å¤æ‚åº¦ä¸¥æ ¼éªŒè¯
                                elif entity_type == "complexity":
                                    if entity.startswith("O(") and entity.endswith(")") and len(entity) <= 15:
                                        validated_list.append(entity)
                                
                                # å…¶ä»–ç±»å‹åŸºæœ¬éªŒè¯
                                elif (2 <= len(entity) <= 15 and
                                      not any(word in entity for word in ["æ‰€ä»¥", "é‡ç‚¹", "å°±æ˜¯", "å…¶å®", "è¿™æ˜¯", "ä¹Ÿå°±æ˜¯"])):
                                    validated_list.append(entity)
                        
                        if validated_list:
                            validated_entities[entity_type] = validated_list
                    
                    result["refined_entities"] = validated_entities
                
                return result
            else:
                print("âš ï¸ æ— æ³•ä»LLMå“åº”ä¸­æå–JSON")
                return {}
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            return {}

    def merge_extraction_results(self, spacy_entities: Dict[str, List[str]], 
                                dictionary_entities: Dict[str, List[str]],
                                llm_result: Dict, source_file: str = None) -> KnowledgePoint:
        """åˆå¹¶å¤šç§æå–ç»“æœ"""
        
        name = llm_result.get("name", "Unknown")
        alias = llm_result.get("alias", [])
        entity_type = llm_result.get("type", "core_concept")
        description = llm_result.get("description", "")
        key_concepts = llm_result.get("key_concepts", [])
        applications = llm_result.get("applications", [])
        
        # åˆå¹¶æ‰€æœ‰å®ä½“
        final_entities = {}
        algorithm_labels = {
            "ALGORITHM_PARADIGM", "DATA_STRUCTURE", "SPECIFIC_ALGORITHM",
            "PROBLEM_TYPE", "CORE_CONCEPT", "COMPLEXITY", "TECHNIQUE"
        }
        
        # 1. ä¼˜å…ˆä½¿ç”¨è¯å…¸ç»“æœï¼ˆæœ€å¯é ï¼‰
        for label, entities in dictionary_entities.items():
            if label.upper() in algorithm_labels:
                final_entities[label.upper()] = entities.copy()
        
        # 2. åŠ å…¥LLMç²¾ç‚¼çš„å®ä½“
        refined_entities = llm_result.get("refined_entities", {})
        for entity_type_key, entity_list in refined_entities.items():
            label = entity_type_key.upper()
            if label in algorithm_labels:
                if label not in final_entities:
                    final_entities[label] = []
                for entity in entity_list:
                    if entity not in final_entities[label]:
                        final_entities[label].append(entity)
        
        # 3. è¡¥å……spaCyç»“æœï¼ˆç»è¿‡æ¸…ç†ï¼‰
        cleaned_spacy = self.clean_entities(spacy_entities)
        for label, entities in cleaned_spacy.items():
            if label in algorithm_labels:
                if label not in final_entities:
                    final_entities[label] = []
                for entity in entities:
                    if entity not in final_entities[label]:
                        final_entities[label].append(entity)
        
        # ç”Ÿæˆå”¯ä¸€ID
        id_prefix = self.generate_id_prefix(entity_type)
        kp_id = f"{id_prefix}{abs(hash(name)) % 1000:03d}"
        
        return KnowledgePoint(
            id=kp_id,
            name=name,
            alias=alias,
            type=entity_type,
            description=description,
            key_concepts=key_concepts,
            applications=applications,
            source_files=[source_file] if source_file else [],
            spacy_entities=final_entities
        )

    def generate_id_prefix(self, type_str: str) -> str:
        """ç”ŸæˆIDå‰ç¼€"""
        prefixes = {
            EntityLabels.ALGORITHM_PARADIGM.value: "AP",
            EntityLabels.DATA_STRUCTURE.value: "DS", 
            EntityLabels.SPECIFIC_ALGORITHM.value: "SA",
            EntityLabels.PROBLEM_TYPE.value: "PT",
            EntityLabels.CORE_CONCEPT.value: "CC",
            EntityLabels.TECHNIQUE.value: "TQ"
        }
        return prefixes.get(type_str, "KP")

    async def extract_knowledge_point(self, text: str, source_file: str = None) -> KnowledgePoint:
        """ä¸»è¦çš„çŸ¥è¯†ç‚¹æå–æ–¹æ³•"""
        print(f"ğŸ” å¼€å§‹ç²¾ç¡®æå–çŸ¥è¯†ç‚¹: {source_file or 'unknown'}")
        
        start_time = datetime.now()
        
        # Step 1: spaCyè§„åˆ™æå–
        print("  ğŸ“ Step 1: spaCyè§„åˆ™æå–...")
        spacy_entities = self.extract_with_spacy_rules(text)
        
        # Step 2: ç²¾ç¡®è¯å…¸åŒ¹é…ï¼ˆæœ€å¯é ï¼‰
        print("  ğŸ“š Step 2: ç²¾ç¡®è¯å…¸åŒ¹é…...")
        dictionary_entities = self.extract_by_precise_dictionary(text)
        
        # Step 3: LLMç²¾ç¡®å¢å¼º
        print("  ğŸ§  Step 3: LLMç²¾ç¡®å¢å¼º...")
        llm_result = await self.enhance_with_precise_llm(text, spacy_entities)
        
        # Step 4: æ™ºèƒ½åˆå¹¶ç»“æœ
        print("  ğŸ”— Step 4: æ™ºèƒ½åˆå¹¶ç»“æœ...")
        knowledge_point = self.merge_extraction_results(
            spacy_entities, dictionary_entities, llm_result, source_file
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        print(f"  âœ… å®Œæˆæå–: {knowledge_point.name} ({processing_time:.2f}s)")
        
        return knowledge_point

class JsonFileManager:
    """JSONæ–‡ä»¶ç®¡ç†å™¨ï¼Œå¤„ç†æ™ºèƒ½æ–‡ä»¶åˆå¹¶"""
    
    def __init__(self, output_dir: str = "knowledge_points_json"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.qwen_client = QwenClient()

    def extract_base_name(self, txt_filename: str) -> str:
        """æå–æ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆå»æ‰åç¼€æ ‡è¯†ï¼‰"""
        base_name = Path(txt_filename).stem
        
        # åç¼€æ¨¡å¼
        suffix_patterns = [
            r'_kama$', r'_wiki$', r'_leetcode$', r'_csdn$', r'_blog$',
            r'_å®˜æ–¹$', r'_è¯¦è§£$', r'_åŸºç¡€$', r'_è¿›é˜¶$', r'_\d+$', r'_[a-zA-Z]+$'
        ]
        
        for pattern in suffix_patterns:
            if re.search(pattern, base_name, re.IGNORECASE):
                base_name = re.sub(pattern, '', base_name, flags=re.IGNORECASE)
                break
        
        return base_name

    def get_json_filename(self, txt_filename: str) -> str:
        """æ ¹æ®txtæ–‡ä»¶åç”Ÿæˆjsonæ–‡ä»¶å"""
        base_name = self.extract_base_name(txt_filename)
        return f"{base_name}.json"

    def json_file_exists(self, json_filename: str) -> bool:
        """æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return (self.output_dir / json_filename).exists()

    def load_existing_json(self, json_filename: str) -> Optional[Dict]:
        """åŠ è½½å·²å­˜åœ¨çš„JSONæ–‡ä»¶"""
        json_path = self.output_dir / json_filename
        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âŒ åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

    async def merge_knowledge_points(self, existing_kp_dict: Dict, new_kp: KnowledgePoint, 
                                   new_source_file: str) -> Dict:
        """ä½¿ç”¨LLMåˆå¹¶çŸ¥è¯†ç‚¹"""
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯çŸ¥è¯†ç‚¹åˆå¹¶ä¸“å®¶ã€‚å°†æ–°çš„çŸ¥è¯†ç‚¹ä¿¡æ¯ä¸ç°æœ‰çŸ¥è¯†ç‚¹æ™ºèƒ½åˆå¹¶ï¼Œä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå½¢æˆæ›´å®Œæ•´å‡†ç¡®çš„çŸ¥è¯†ç‚¹æè¿°ã€‚"
            },
            {
                "role": "user",
                "content": f"""
ç°æœ‰çŸ¥è¯†ç‚¹ï¼š
{json.dumps(existing_kp_dict, ensure_ascii=False, indent=2)}

æ–°çš„çŸ¥è¯†ç‚¹ä¿¡æ¯ï¼š
{json.dumps(new_kp.to_dict(), ensure_ascii=False, indent=2)}

æ–°æ¥æºæ–‡ä»¶ï¼š{new_source_file}

åˆå¹¶è§„åˆ™ï¼š
1. ä¿æŒåŸæœ‰çš„idå’Œnameä¸å˜
2. åˆå¹¶aliasåˆ—è¡¨ï¼Œå»é‡ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ„ä¹‰çš„åˆ«å
3. ç»¼åˆä¸¤ä¸ªæè¿°ï¼Œç”Ÿæˆæ›´å®Œæ•´è¯¦ç»†çš„description
4. åˆå¹¶key_conceptså’Œapplicationsï¼Œå»é‡ï¼Œä¿æŒé€»è¾‘æ€§
5. åˆå¹¶source_filesåˆ—è¡¨ï¼Œè®°å½•æ‰€æœ‰æ¥æº
6. åˆå¹¶spacy_entitiesï¼Œå»é‡ä½†ä¿ç•™æœ‰ä»·å€¼çš„å®ä½“
7. æ›´æ–°updated_atæ—¶é—´æˆ³

è¯·è¿”å›å®Œæ•´çš„åˆå¹¶åçš„JSONï¼Œæ ¼å¼ä¸è¾“å…¥ä¿æŒä¸€è‡´ã€‚
åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
            }
        ]
        
        response = await self.qwen_client.chat_completion_async(messages, model="qwen-max")
        
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response[json_start:json_end]
                merged_dict = json.loads(json_str)
                merged_dict["updated_at"] = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
                return merged_dict
            else:
                return self.manual_merge(existing_kp_dict, new_kp, new_source_file)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨åˆå¹¶: {e}")
            return self.manual_merge(existing_kp_dict, new_kp, new_source_file)

    def manual_merge(self, existing_kp_dict: Dict, new_kp: KnowledgePoint, 
                    new_source_file: str) -> Dict:
        """æ‰‹åŠ¨åˆå¹¶çŸ¥è¯†ç‚¹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        merged = existing_kp_dict.copy()
        new_dict = new_kp.to_dict()
        
        # åˆå¹¶å„ç§å­—æ®µ
        for field in ["alias", "key_concepts", "applications"]:
            existing_set = set(merged.get(field, []))
            new_set = set(new_dict.get(field, []))
            merged[field] = list(existing_set | new_set)
        
        # åˆå¹¶source_files
        existing_sources = set(merged.get("source_files", []))
        existing_sources.add(new_source_file)
        merged["source_files"] = list(existing_sources)
        
        # åˆå¹¶spacy_entities
        existing_entities = merged.get("spacy_entities", {})
        new_entities = new_dict.get("spacy_entities", {})
        for entity_type, entity_list in new_entities.items():
            if entity_type not in existing_entities:
                existing_entities[entity_type] = []
            existing_entities[entity_type] = list(set(existing_entities[entity_type] + entity_list))
        merged["spacy_entities"] = existing_entities
        
        # æ™ºèƒ½åˆå¹¶æè¿°
        old_desc = merged.get("description", "")
        new_desc = new_dict.get("description", "")
        if len(new_desc) > len(old_desc) * 1.5:
            merged["description"] = new_desc
        elif len(new_desc) > 50 and old_desc not in new_desc and new_desc not in old_desc:
            merged["description"] = f"{old_desc}\n\nè¡¥å……ä¿¡æ¯ï¼š{new_desc}"
        
        merged["updated_at"] = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        return merged

    def save_knowledge_point(self, knowledge_point_dict: Dict, json_filename: str):
        """ä¿å­˜çŸ¥è¯†ç‚¹åˆ°JSONæ–‡ä»¶"""
        json_path = self.output_dir / json_filename
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(knowledge_point_dict, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜æˆåŠŸ: {json_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    async def process_knowledge_point(self, knowledge_point: KnowledgePoint, 
                                    source_file: str) -> str:
        """å¤„ç†çŸ¥è¯†ç‚¹ï¼šæ–°å»ºæˆ–åˆå¹¶"""
        json_filename = self.get_json_filename(source_file)
        
        if self.json_file_exists(json_filename):
            existing_dict = self.load_existing_json(json_filename)
            if existing_dict:
                # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«æ­¤æºæ–‡ä»¶
                existing_sources = existing_dict.get("source_files", [])
                if source_file in existing_sources:
                    print(f"â­ï¸ æ–‡ä»¶å·²å¤„ç†è¿‡ï¼Œè·³è¿‡: {source_file}")
                    return "skipped"
                
                print(f"ğŸ”„ å‘ç°ç›¸å…³JSONæ–‡ä»¶ï¼Œå‡†å¤‡åˆå¹¶: {json_filename}")
                merged_dict = await self.merge_knowledge_points(
                    existing_dict, knowledge_point, source_file
                )
                self.save_knowledge_point(merged_dict, json_filename)
                sources = merged_dict.get('source_files', [])
                print(f"âœ… åˆå¹¶å®Œæˆ: {merged_dict['name']} (æ¥æº: {', '.join(sources)})")
                return "merged"
        
        # æ–°å»ºæ–‡ä»¶
        kp_dict = knowledge_point.to_dict()
        if source_file not in kp_dict["source_files"]:
            kp_dict["source_files"].append(source_file)
        
        self.save_knowledge_point(kp_dict, json_filename)
        print(f"âœ… æ–°å»ºå®Œæˆ: {knowledge_point.name}")
        return "created"

class MainProcessor:
    """ä¸»å¤„ç†å™¨"""
    
    def __init__(self, input_dir: str = r"F:\My_project\programmercarl_articles\knowledepoints", 
                 output_dir: str = "knowledge_points_json"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.extractor = PreciseEntityExtractor()
        self.json_manager = JsonFileManager(output_dir)
        
        if not self.input_dir.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")

    def scan_txt_files(self) -> List[Path]:
        """æ‰«ætxtæ–‡ä»¶"""
        txt_files = list(self.input_dir.rglob("*.txt"))
        print(f"ğŸ” å‘ç° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
        return txt_files

    def read_txt_file(self, file_path: Path) -> str:
        """è¯»å–txtæ–‡ä»¶å†…å®¹"""
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read().strip()
            except UnicodeDecodeError:
                continue
        
        raise Exception(f"æ— æ³•è§£ç æ–‡ä»¶ {file_path}")

    def preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬å†…å®¹"""
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = content.strip()
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        content = re.sub(r'[ \t]+', ' ', content)
        return content

    def validate_content(self, content: str, file_path: Path) -> bool:
        """éªŒè¯æ–‡ä»¶å†…å®¹è´¨é‡"""
        if len(content) < 50:
            print(f"âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡: {file_path.name}")
            return False
        
        algorithm_keywords = [
            'ç®—æ³•', 'æ•°æ®ç»“æ„', 'æ’åº', 'æŸ¥æ‰¾', 'æ ‘', 'å›¾', 'åŠ¨æ€è§„åˆ’', 'dp',
            'algorithm', 'complexity', 'é€’å½’', 'è¿­ä»£', 'é“¾è¡¨', 'æ•°ç»„', 'æ ˆ', 'é˜Ÿåˆ—'
        ]
        
        has_algorithm_content = any(keyword in content.lower() for keyword in algorithm_keywords)
        if not has_algorithm_content:
            print(f"âš ï¸ æœªæ£€æµ‹åˆ°ç®—æ³•ç›¸å…³å†…å®¹: {file_path.name}")
        
        return True

    async def process_single_file(self, file_path: Path) -> bool:
        """å¤„ç†å•ä¸ªtxtæ–‡ä»¶"""
        try:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # è¯»å–æ–‡ä»¶
            content = self.read_txt_file(file_path)
            
            # é¢„å¤„ç†
            processed_content = self.preprocess_content(content)
            
            # éªŒè¯å†…å®¹
            if not self.validate_content(processed_content, file_path):
                return False
            
            # æå–çŸ¥è¯†ç‚¹
            knowledge_point = await self.extractor.extract_knowledge_point(
                processed_content, 
                file_path.name
            )
            
            # å¤„ç†JSONæ–‡ä»¶ï¼ˆæ–°å»ºæˆ–åˆå¹¶ï¼‰
            result = await self.json_manager.process_knowledge_point(
                knowledge_point, 
                file_path.name
            )
            
            print(f"ğŸ¯ å¤„ç†ç»“æœ: {result}")
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")
            return False

    def group_files_by_base_name(self, txt_files: List[Path]) -> Dict[str, List[Path]]:
        """æŒ‰åŸºç¡€åç§°åˆ†ç»„æ–‡ä»¶"""
        file_groups = {}
        
        for file_path in txt_files:
            base_name = self.json_manager.extract_base_name(file_path.name)
            
            if base_name not in file_groups:
                file_groups[base_name] = []
            file_groups[base_name].append(file_path)
        
        # æ˜¾ç¤ºåˆ†ç»„ä¿¡æ¯
        print(f"\nğŸ“Š æ–‡ä»¶åˆ†ç»„ç»“æœ:")
        for base_name, files in file_groups.items():
            if len(files) > 1:
                print(f"  ğŸ”— {base_name}: {len(files)} ä¸ªæ–‡ä»¶")
                for file in files:
                    print(f"    - {file.name}")
            else:
                print(f"  ğŸ“„ {base_name}: 1 ä¸ªæ–‡ä»¶ ({files[0].name})")
        
        return file_groups

    async def process_all_files(self, max_files: int = None, skip_processed: bool = True) -> Dict:
        """å¤„ç†æ‰€æœ‰txtæ–‡ä»¶"""
        print(f"ğŸš€ ç®—æ³•çŸ¥è¯†ç‚¹ç²¾ç¡®æå–ç³»ç»Ÿ v2.1")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("=" * 60)
        
        # æ‰«ææ–‡ä»¶
        txt_files = self.scan_txt_files()
        
        if max_files:
            txt_files = txt_files[:max_files]
            print(f"ğŸ”¢ é™åˆ¶å¤„ç†æ–‡ä»¶æ•°: {max_files}")
        
        # æŒ‰åŸºç¡€åç§°åˆ†ç»„æ–‡ä»¶
        file_groups = self.group_files_by_base_name(txt_files)
        print(f"ğŸ”— æ£€æµ‹åˆ° {len(file_groups)} ä¸ªçŸ¥è¯†ç‚¹ç»„ï¼Œå…± {sum(len(group) for group in file_groups.values())} ä¸ªæ–‡ä»¶")
        
        # å¤„ç†ç»Ÿè®¡
        stats = {
            "total_files": len(txt_files),
            "total_groups": len(file_groups),
            "processed_files": 0,
            "failed_files": 0,
            "created_json": 0,
            "merged_json": 0,
            "skipped_files": 0,
            "failed_files_list": [],
            "processing_time": 0
        }
        
        start_time = datetime.now()
        
        # æŒ‰ç»„å¤„ç†æ–‡ä»¶
        group_index = 0
        for base_name, file_group in file_groups.items():
            group_index += 1
            print(f"\n[ç»„ {group_index}/{len(file_groups)}] å¤„ç†çŸ¥è¯†ç‚¹: {base_name}")
            print(f"ğŸ“„ ç›¸å…³æ–‡ä»¶: {[f.name for f in file_group]}")
            print("=" * 50)
            
            # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œä¼˜å…ˆå¤„ç†ä¸»è¦æ–‡ä»¶
            file_group.sort(key=lambda f: f.stat().st_size, reverse=True)
            
            for i, file_path in enumerate(file_group, 1):
                print(f"\n  [{i}/{len(file_group)}] å¤„ç†æ–‡ä»¶: {file_path.name}")
                
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
                json_filename = self.json_manager.get_json_filename(file_path.name)
                if skip_processed and self.json_manager.json_file_exists(json_filename):
                    existing_json = self.json_manager.load_existing_json(json_filename)
                    if existing_json and file_path.name in existing_json.get("source_files", []):
                        print(f"    â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                        stats["skipped_files"] += 1
                        continue
                
                try:
                    success = await self.process_single_file(file_path)
                    
                    if success:
                        stats["processed_files"] += 1
                        
                        # æ£€æŸ¥æ˜¯æ–°å»ºè¿˜æ˜¯åˆå¹¶
                        json_path = self.output_dir / json_filename
                        if json_path.exists():
                            existing_json = self.json_manager.load_existing_json(json_filename)
                            if existing_json and len(existing_json.get("source_files", [])) > 1:
                                stats["merged_json"] += 1
                            else:
                                stats["created_json"] += 1
                    else:
                        stats["failed_files"] += 1
                        stats["failed_files_list"].append({
                            "file": file_path.name,
                            "reason": "å¤„ç†å¤±è´¥"
                        })
                        
                except Exception as e:
                    print(f"    âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸ {file_path.name}: {e}")
                    stats["failed_files"] += 1
                    stats["failed_files_list"].append({
                        "file": file_path.name,
                        "reason": str(e)
                    })
        
        stats["processing_time"] = (datetime.now() - start_time).total_seconds()
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.generate_report(stats)
        
        return stats

    def generate_report(self, stats: Dict):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        print(f"\n" + "="*60)
        print("ğŸ“Š å¤„ç†å®ŒæˆæŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {stats['processing_time']:.1f} ç§’")
        print()
        print(f"ğŸ“„ æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"ğŸ”— çŸ¥è¯†ç‚¹ç»„æ•°: {stats['total_groups']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {stats['failed_files']}")
        print(f"â­ï¸ è·³è¿‡æ–‡ä»¶: {stats['skipped_files']}")
        print()
        print(f"ğŸ†• æ–°å»ºJSON: {stats['created_json']}")
        print(f"ğŸ”„ åˆå¹¶JSON: {stats['merged_json']}")
        
        if stats["failed_files_list"]:
            print(f"\nâŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for failed in stats["failed_files_list"]:
                print(f"   - {failed['file']}: {failed['reason']}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = self.output_dir / f"processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        except Exception as e:
            print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

    def list_generated_files(self):
        """åˆ—å‡ºç”Ÿæˆçš„JSONæ–‡ä»¶"""
        json_files = list(self.output_dir.glob("*.json"))
        # è¿‡æ»¤æ‰æŠ¥å‘Šæ–‡ä»¶
        json_files = [f for f in json_files if not f.name.startswith("processing_report_")]
        
        print(f"\nğŸ“š å·²ç”Ÿæˆçš„JSONæ–‡ä»¶ ({len(json_files)}ä¸ª):")
        
        for json_file in sorted(json_files):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    name = data.get("name", "Unknown")
                    sources = data.get("source_files", [])
                    problem_types = data.get("spacy_entities", {}).get("PROBLEM_TYPE", [])
                    
                    print(f"   ğŸ“„ {json_file.name}")
                    print(f"      ğŸ·ï¸ åç§°: {name}")
                    print(f"      ğŸ“ æ¥æº({len(sources)}): {', '.join(sources)}")
                    if problem_types:
                        print(f"      ğŸ¯ é—®é¢˜ç±»å‹: {', '.join(problem_types)}")
                    print()
            except Exception as e:
                print(f"   âŒ {json_file.name}: è¯»å–å¤±è´¥ ({e})")

# é…ç½®æ–‡ä»¶åˆ›å»ºå‡½æ•°
def create_default_patterns_file():
    """åˆ›å»ºé»˜è®¤çš„patterns.jsonæ–‡ä»¶"""
    patterns_file = Path("patterns.json")
    
    if patterns_file.exists():
        print(f"âœ… patterns.json å·²å­˜åœ¨")
        return
    
    default_patterns = [
        # ç®—æ³•èŒƒå¼
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "åŠ¨æ€è§„åˆ’"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "dp"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "è´ªå¿ƒ"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "è´ªå¿ƒç®—æ³•"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "åˆ†æ²»"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "åˆ†æ²»ç®—æ³•"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "å›æº¯"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "å›æº¯ç®—æ³•"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "æ·±åº¦ä¼˜å…ˆæœç´¢"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "dfs"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "å¹¿åº¦ä¼˜å…ˆæœç´¢"}]},
        {"label": "ALGORITHM_PARADIGM", "pattern": [{"LOWER": "bfs"}]},
        
        # æ•°æ®ç»“æ„
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ•°ç»„"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "é“¾è¡¨"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ ˆ"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "é˜Ÿåˆ—"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "æ ‘"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "å›¾"}]},
        {"label": "DATA_STRUCTURE", "pattern": [{"LOWER": "å“ˆå¸Œè¡¨"}]},
        
        # å…·ä½“ç®—æ³•
        {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "å¿«é€Ÿæ’åº"}]},
        {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "å½’å¹¶æ’åº"}]},
        {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "äºŒåˆ†æŸ¥æ‰¾"}]},
        {"label": "SPECIFIC_ALGORITHM", "pattern": [{"LOWER": "kmp"}]},
        
        # æŠ€å·§
        {"label": "TECHNIQUE", "pattern": [{"LOWER": "åŒæŒ‡é’ˆ"}]},
        {"label": "TECHNIQUE", "pattern": [{"LOWER": "æ»‘åŠ¨çª—å£"}]},
        {"label": "TECHNIQUE", "pattern": [{"LOWER": "å‰ç¼€å’Œ"}]},
        
        # æ ¸å¿ƒæ¦‚å¿µ
        {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "æ—¶é—´å¤æ‚åº¦"}]},
        {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "ç©ºé—´å¤æ‚åº¦"}]},
        {"label": "CORE_CONCEPT", "pattern": [{"LOWER": "é€’å½’"}]},
    ]
    
    try:
        with open(patterns_file, 'w', encoding='utf-8') as f:
            json.dump(default_patterns, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ›å»ºé»˜è®¤patterns.jsonæ–‡ä»¶ï¼Œå…± {len(default_patterns)} ä¸ªæ¨¡å¼")
    except Exception as e:
        print(f"âŒ åˆ›å»ºpatterns.jsonå¤±è´¥: {e}")

# æµ‹è¯•å‡½æ•°
async def test_single_file():
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†"""
    test_content = """
# 01èƒŒåŒ…é—®é¢˜

01èƒŒåŒ…é—®é¢˜æ˜¯åŠ¨æ€è§„åˆ’ä¸­çš„ç»å…¸é—®é¢˜ä¹‹ä¸€ã€‚

## é—®é¢˜æè¿°
ç»™å®šnä¸ªç‰©å“ï¼Œæ¯ä¸ªç‰©å“æœ‰é‡é‡å’Œä»·å€¼ï¼ŒèƒŒåŒ…å®¹é‡ä¸ºWï¼Œæ±‚æœ€å¤§ä»·å€¼ã€‚

## è§£æ³•æ€è·¯
ä½¿ç”¨åŠ¨æ€è§„åˆ’ï¼ŒçŠ¶æ€è½¬ç§»æ–¹ç¨‹ä¸ºï¼šdp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]] + value[i])

## æ—¶é—´å¤æ‚åº¦
O(nW)ï¼Œå…¶ä¸­næ˜¯ç‰©å“æ•°é‡ï¼ŒWæ˜¯èƒŒåŒ…å®¹é‡ã€‚

## åº”ç”¨åœºæ™¯
- èµ„æºåˆ†é…é—®é¢˜
- æŠ•èµ„ç»„åˆä¼˜åŒ–
"""
    
    print("ğŸ§ª æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†...")
    
    extractor = PreciseEntityExtractor()
    kp = await extractor.extract_knowledge_point(test_content, "test_01èƒŒåŒ….txt")
    
    print("\nğŸ“‹ æå–ç»“æœ:")
    print(json.dumps(kp.to_dict(), ensure_ascii=False, indent=2))

def test_base_name_extraction():
    """æµ‹è¯•åŸºç¡€åç§°æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ–‡ä»¶åæå–åŠŸèƒ½")
    print("=" * 40)
    
    json_manager = JsonFileManager()
    
    test_files = [
        "BFS_kama.txt", "BFS_wiki.txt", "DFS_leetcode.txt", "DFS_å®˜æ–¹.txt",
        "åŠ¨æ€è§„åˆ’_è¯¦è§£.txt", "åŠ¨æ€è§„åˆ’_åŸºç¡€.txt", "KMP_blog.txt", "KMP_csdn.txt",
        "èƒŒåŒ…é—®é¢˜.txt", "01èƒŒåŒ…_1.txt", "01èƒŒåŒ…_2.txt"
    ]
    
    base_name_groups = {}
    
    for filename in test_files:
        base_name = json_manager.extract_base_name(filename)
        json_filename = json_manager.get_json_filename(filename)
        
        if base_name not in base_name_groups:
            base_name_groups[base_name] = []
        base_name_groups[base_name].append(filename)
        
        print(f"ğŸ“„ {filename} -> {base_name} -> {json_filename}")
    
    print(f"\nğŸ“Š åˆ†ç»„ç»“æœ:")
    for base_name, files in base_name_groups.items():
        if len(files) > 1:
            print(f"ğŸ”— {base_name}: {files}")
        else:
            print(f"ğŸ“„ {base_name}: {files[0]}")

def view_existing_json():
    """æŸ¥çœ‹ç°æœ‰çš„JSONæ–‡ä»¶"""
    output_dir = Path("knowledge_points_json")
    if not output_dir.exists():
        print("ğŸ“ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
        return
    
    json_files = list(output_dir.glob("*.json"))
    json_files = [f for f in json_files if not f.name.startswith("processing_report_")]
    
    if not json_files:
        print("ğŸ“ æš‚æ— JSONæ–‡ä»¶")
        return
    
    print(f"ğŸ“š ç°æœ‰JSONæ–‡ä»¶ ({len(json_files)}ä¸ª):")
    for json_file in sorted(json_files):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"\nğŸ“„ {json_file.name}")
                print(f"   ğŸ·ï¸ åç§°: {data.get('name', 'Unknown')}")
                print(f"   ğŸ“ ç±»å‹: {data.get('type', 'Unknown')}")
                print(f"   ğŸ“ æ¥æº: {', '.join(data.get('source_files', []))}")
                
                # æ˜¾ç¤ºé—®é¢˜ç±»å‹æå–ç»“æœ
                problem_types = data.get('spacy_entities', {}).get('PROBLEM_TYPE', [])
                if problem_types:
                    print(f"   ğŸ¯ é—®é¢˜ç±»å‹: {', '.join(problem_types)}")
                
                concepts = data.get('key_concepts', [])
                if concepts:
                    print(f"   ğŸ”‘ æ¦‚å¿µ: {', '.join(concepts[:3])}{'...' if len(concepts) > 3 else ''}")
        except Exception as e:
            print(f"   âŒ è¯»å–å¤±è´¥: {e}")

# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®—æ³•çŸ¥è¯†ç‚¹ç²¾ç¡®æå–ç³»ç»Ÿ v2.1")
    print("ğŸ¯ ä¸“æ³¨äºLLM one-shotå‡†ç¡®æ€§")
    print("=" * 50)
    
    # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
    create_default_patterns_file()
    
    # åˆå§‹åŒ–ä¸»å¤„ç†å™¨
    try:
        processor = MainProcessor(
            input_dir=r"F:\algokg_platform\data\raw\programmercarl_articles\knowledepoints",
            output_dir=r"F:\algokg_platform\data\raw\knowledgePoint_extraction"
        )
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        print("è¯·ç¡®è®¤è¾“å…¥ç›®å½•è·¯å¾„æ­£ç¡®")
        return
    
    print(f"\nğŸ”§ å½“å‰æå–æ–¹æ³•ç»„åˆ:")
    print(f"  ğŸ“ spaCyè§„åˆ™åŒ¹é…: âœ…")
    print(f"  ğŸ“š ç²¾ç¡®è¯å…¸åŒ¹é…: âœ…") 
    print(f"  ğŸ§  LLMç²¾ç¡®å¢å¼º: âœ…")
    print(f"  ğŸ”§ æ™ºèƒ½åå¤„ç†: âœ…")
    
    try:
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        stats = await processor.process_all_files(
            max_files=None,  # ä¸é™åˆ¶æ–‡ä»¶æ•°é‡
            skip_processed=True  # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
        )
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        processor.list_generated_files()
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
        print(f"âœ… æˆåŠŸ: {stats['processed_files']}")
        print(f"âŒ å¤±è´¥: {stats['failed_files']}")
        print(f"ğŸ†• æ–°å»º: {stats['created_json']}")
        print(f"ğŸ”„ åˆå¹¶: {stats['merged_json']}")
        print(f"ğŸ”— çŸ¥è¯†ç‚¹ç»„: {stats['total_groups']}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        if command == "test":
            # æµ‹è¯•å•ä¸ªæ–‡ä»¶
            asyncio.run(test_single_file())
        elif command == "view":
            # æŸ¥çœ‹ç°æœ‰æ–‡ä»¶
            view_existing_json()
        elif command == "patterns":
            # åˆ›å»ºpatternsæ–‡ä»¶
            create_default_patterns_file()
        elif command == "test-names":
            # æµ‹è¯•æ–‡ä»¶åæå–
            test_base_name_extraction()
        else:
            print("å¯ç”¨å‘½ä»¤:")
            print("  test       - æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤„ç†")
            print("  view       - æŸ¥çœ‹ç°æœ‰JSONæ–‡ä»¶")
            print("  patterns   - åˆ›å»ºé»˜è®¤patterns.json")
            print("  test-names - æµ‹è¯•æ–‡ä»¶åæå–åŠŸèƒ½")
    else:
        # è¿è¡Œä¸»ç¨‹åº
        asyncio.run(main())